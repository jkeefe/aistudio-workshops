{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mayoral USE searcher for NICAR",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "co7MV6sX7Xto"
      },
      "source": [
        "# Document Search with Sentence Encoder\n",
        "\n",
        "by Jeremy Merrill, Quartz.\n",
        "\n",
        "for NICAR 2020.\n",
        "\n",
        "Imagine getting a huge pile of documents. You know that there's interesting stuff in the pile, but you don't know what, exactly. \n",
        "\n",
        "Let's search that pile.\n",
        "\n",
        "Github repo: https://github.com/Quartz/aistudio-searching-data-dumps-with-use / https://github.com/Quartz/aistudio-workshops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "63Pd3nJnTl-i"
      },
      "source": [
        "**IMPORTANT** Note: Please select \"**Python 3**\" _and_ \"**GPU**\" in the ***Runtime->Change Runtime type*** dropdown menu above _before_ running this notebook for faster execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOTzp8O36CyQ"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "This is an interactive demo. You can run all the code necessary to search a pile of documents right here. (A medium-sized, since we don't have all day.)\n",
        "\n",
        "We're using two neat pieces of technology called the *Universal Sentence Encoder* and *Annoy*.\n",
        "\n",
        "- the *Universal Sentence Encoder* is a pre-trained machine-learning model that sorta understands human language. If you feed in a sentence, it comes out with 512 numbers that represent the approximate meaning of that sentence. What's really cool is that if you feed in a second sentence that means about the same thing, that second sentence's numbers will be very close to those of the first sentence.\n",
        "- *Annoy* is a library that makes it really easy to find points in vector space that are close to each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWrZqHVFBGfW",
        "colab_type": "text"
      },
      "source": [
        "What's \"vector space\"? Imagine dot plot with an x-axis and a y-axis. That's two-dimensional vector space.\n",
        "\n",
        "This is three-dimensional vector space. Three axes: x, y, z.\n",
        "\n",
        "![alt text](https://filedn.com/lVaAxkskVxILBoUDG3XUrm7/nicar20presentation/Screen%20Shot%202020-02-28%20at%205.43.59%20PM.png)\n",
        "\n",
        "Now imagine 512 axes. That's what we're dealing with here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZPVY12mDb0P",
        "colab_type": "text"
      },
      "source": [
        "## Okay, let's get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "lVjNK8shFKOC",
        "outputId": "78193ad1-6fd3-487c-84fe-7c09b8d0549f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#@title Setup Environment\n",
        "#latest Tensorflow that supports sentencepiece is 1.14\n",
        "!pip uninstall --quiet --yes tensorflow\n",
        "!pip install --quiet tensorflow-gpu==1.14\n",
        "!pip install --quiet tensorflow==1.14\n",
        "!pip install --quiet tensorflow-hub\n",
        "!pip install --quiet bokeh\n",
        "!pip install --quiet tf-sentencepiece\n",
        "!pip install --quiet annoy\n",
        "!pip install --quiet tqdm\n",
        "!pip install --quiet w3lib\n",
        "!pip install --quiet syntok"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 377.0MB 42kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 25.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 26.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 109.2MB 27kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MSeY-MUQo2Ha",
        "colab": {}
      },
      "source": [
        "#@title Setup common imports and functions\n",
        "%tensorflow_version 1.x\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "from annoy import AnnoyIndex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gk2IRjZFGDsK"
      },
      "source": [
        "This is additional boilerplate code where we import the pre-trained ML model we will use to encode text throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mkmF3w8WGLcM",
        "cellView": "both",
        "outputId": "bc318d98-c621-4354-ae39-9c77478ca7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title get the machine learning stuff set up. (boilerplate!)\n",
        "# this version of the Universal Sentence Encoder only \"speaks\" English\n",
        "# but there's another version you can switch in that supports 16 different languages!\n",
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
        "\n",
        "# boilerplate, getting started with Tensorflow.\n",
        "# (how to use Tensorflow is way outside the scope of this class)\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  multiling_embed = hub.Module(module_url)\n",
        "  embedded_text = multiling_embed(text_input)\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n",
        "\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XjNsxF2b7ZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @ get the data.\n",
        "# let's get our data!\n",
        "# it's a JSONL file, which is a file with one page, as its own JSON document, per line.\n",
        "!wget --quiet -nc -O nyc_docs.jsonl https://raw.githubusercontent.com/Quartz/aistudio-searching-data-dumps-with-use/master/nyc_docs.jsonl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eExMTHZNGn86",
        "colab_type": "text"
      },
      "source": [
        "## What the heck is JSONL?\n",
        "\n",
        "Don't worry too much about it. It looks like this, but there's nothing special to it, it's just a way to get the content of the pages in [these emails](https://github.com/Quartz/aistudio-doc2vec-for-investigative-journalism/blob/master/2018.05.24_BerlinRosen_Responsive_Records.pdf): \n",
        "\n",
        "![alt text](https://filedn.com/lVaAxkskVxILBoUDG3XUrm7/nicar20presentation/Screen%20Shot%202020-03-03%20at%2011.01.44%20AM.png)\n",
        "\n",
        "```\n",
        "{\"_source\": {\"content\": \"From:Dan Levitan\n",
        "To:Grybauskas, Natalie\n",
        "Subject:RE: Groundbreaking\n",
        "Date:Thursday, February 11, 2016 11:51:00 AM\n",
        "The Pizza place Link is fine, the 14\n",
        "th st. one is not.\n",
        " --Dan Levitan\n",
        " BerlinRosen Public Affairs\"}, \"_id\": \"p1938\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVmKpe5hnsxx",
        "colab_type": "text"
      },
      "source": [
        "## Chopping each page into a list of sentences\n",
        "\n",
        "We have to do this because pages and paragraphs often cover multiple topics, which might confuse the model. And, Universal Sentence Encoder is built to encode sentences... and so it ignores anything after the 128th word in its input.\n",
        "\n",
        "The code below cuts the text into sentences, but groups any two consecutive sentences under 15 words long together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEh77ZORnrrr",
        "colab_type": "code",
        "outputId": "af8e9b4b-49d5-4232-96cb-8a8ab3083e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# takes about 10 seconds\n",
        "\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from functools import reduce\n",
        "from w3lib.html import remove_tags\n",
        "\n",
        "import syntok.segmenter as segmenter\n",
        "\n",
        "total_docs = 4251 # get this with `wc` (only used for progress bar)\n",
        "\n",
        "total_short_paragraphs = 0\n",
        "MAX_SENT_LEN = 100\n",
        "\n",
        "def sentenceify(text):\n",
        "    return [sl for l in [[''.join([t.spacing + t.value for t in s]) for s in p if len(s) < MAX_SENT_LEN] for p in segmenter.analyze(text)] for sl in l if any(map(lambda x: x.isalpha(), sl))]\n",
        "\n",
        "\n",
        "def clean_html(html):\n",
        "    if \"<\" in html and \">\" in html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "            plist = soup.find('plist')\n",
        "            if plist:\n",
        "                plist.decompose() # remove plists because ugh\n",
        "            text = soup.getText()\n",
        "        except:\n",
        "            text = remove_tags(html)\n",
        "        return '. '.join(text.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
        "    else:\n",
        "        return '. '.join(html.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
        "\n",
        "# if this sentence is short, then group it with other short sentences (so you get groups of continuous short sentences, broken up by one-element groups of longer sentences)\n",
        "def short_sentence_grouper_bean_factory(target_sentence_length): # in chars\n",
        "    def group_short_sentences(list_of_lists_of_sentences, next_sentence):\n",
        "        if not list_of_lists_of_sentences:\n",
        "            return [[next_sentence]]\n",
        "        if len(next_sentence) < target_sentence_length:\n",
        "           list_of_lists_of_sentences[-1].append(next_sentence)\n",
        "        else:\n",
        "            list_of_lists_of_sentences.append([next_sentence])\n",
        "            list_of_lists_of_sentences.append([])\n",
        "        return list_of_lists_of_sentences\n",
        "    return group_short_sentences\n",
        "\n",
        "\n",
        "def overlap(document_tokens, target_length):\n",
        "    \"\"\" pseudo-paginate a document by creating lists of tokens of length `target-length` that overlap at 50%\n",
        "    return a list of `target_length`-length lists of tokens, overlapping by 50% representing all the tokens in the document \n",
        "    \"\"\"\n",
        "\n",
        "    overlapped = []\n",
        "    cursor = 0\n",
        "    while len(' '.join(document_tokens[cursor:]).split()) >= target_length:\n",
        "      overlapped.append(document_tokens[cursor:cursor+target_length])\n",
        "      cursor += target_length // 2\n",
        "    return overlapped\n",
        "\n",
        "\n",
        "def sentences_to_short_paragraphs(group_of_sentences, target_length, min_shingle_length=10):\n",
        "    \"\"\" outputting overlapping groups of shorter sentences \n",
        "    \n",
        "        group_of_sentences = list of strings, where each string is a sentences\n",
        "        target_length = max length IN WORDS of output sentennces\n",
        "        min_shingle_length = don't have sentences that differ just in the inclusion of a sentence of this size\n",
        "    \"\"\"\n",
        "    if len(group_of_sentences) == 1:\n",
        "        return [' '.join(group_of_sentences[0].split())]\n",
        "    sentences_as_words = [sent.split() for sent in group_of_sentences]\n",
        "    sentences_as_words = [sentence for sentence in sentences_as_words if [len(word) for word in sentence].count(1) < (len(sentence) * 0.5) ]\n",
        "    paragraphs = []\n",
        "    for i, sentence in enumerate(sentences_as_words[:-1]):\n",
        "        if i > 0 and len(sentence) < min_shingle_length  and len(sentences_as_words[i-1]) < min_shingle_length and i % 2 == 0:\n",
        "            continue # skip really short sentences if the previous one is also really short (but not so often that we lose anything )\n",
        "        buff = list(sentence) # just making a copy.\n",
        "        for subsequent_sentence in sentences_as_words[i+1:]:\n",
        "            if len(buff) + len(subsequent_sentence) <= target_length:\n",
        "                buff += subsequent_sentence\n",
        "            else:\n",
        "                break\n",
        "        paragraphs.append(buff)\n",
        "    return [' '.join(graf) for graf in paragraphs]\n",
        "\n",
        "\n",
        "def to_short_paragraphs(text, paragraph_len=15, min_sentence_len=8): # paragraph_len in words, min_sentence_len in chars\n",
        "    sentences = sentenceify( clean_html(text) )\n",
        "    grouped_sentences = reduce(short_sentence_grouper_bean_factory(150) , sentences, [])\n",
        "    return [sl for l in [sentences_to_short_paragraphs(group, paragraph_len) for group in grouped_sentences if len(group) >= 2 or (len(group) > 0 and len(group[0]) > min_sentence_len)] for sl in l if sl]\n",
        "\n",
        "paragraph_target_length = 15\n",
        "\n",
        "with open(f\"nyc_docs-sentences{paragraph_target_length}.json\", 'w') as writer: \n",
        "    with open('nyc_docs.jsonl', 'r') as reader:\n",
        "        for i, line_json in tqdm(enumerate(reader), total=total_docs):\n",
        "            line = json.loads(line_json)\n",
        "            text = line[\"_source\"][\"content\"][:1000000]\n",
        "            for j, page in enumerate(to_short_paragraphs(text, paragraph_target_length)):\n",
        "                total_short_paragraphs += 1\n",
        "                writer.write(json.dumps({\n",
        "                    \"text\": page, \n",
        "                    \"_id\": line[\"_id\"], \n",
        "                    \"chonk\": j,\n",
        "                    # \"routing\": line.get(\"_routing\", None),\n",
        "                    # \"path\": line[\"_source\"][\"path\"]\n",
        "                    }) + \"\\n\")\n",
        "print(f\"total paragraphs: {total_short_paragraphs}\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4251/4251 [00:15<00:00, 282.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total paragraphs: 37281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxAFAJI9xsAU"
      },
      "source": [
        "# Creating a Multilingual Semantic-Similarity Search Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3DIT9uT7Z34"
      },
      "source": [
        "## Using a pre-trained model to transform sentences into vectors\n",
        "\n",
        "We compute embeddings in _batches_ so that they fit in the GPU's RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRoRT5qCEIYy",
        "outputId": "46fd3fa1-fd2c-4d6b-8591-be31a8f4f563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Takes about 12 seconds\n",
        "vector_index_chunk = AnnoyIndex(512, 'angular')  # Length of item vector that will be indexed\n",
        "\n",
        "batch_size = 256\n",
        "docs = {}\n",
        "\n",
        "doc_counter = 0\n",
        "with tqdm(total=37281) as pbar:\n",
        "  for j, batch in enumerate(pd.read_json('nyc_docs-sentences15.json', lines=True, chunksize=batch_size)):\n",
        "    batch_vecs = session.run(embedded_text, feed_dict={text_input: batch[\"text\"]})\n",
        "    # sentences.extend(batch[\"text\"])\n",
        "    pbar.update(len(batch))\n",
        "    doc_idxs = list(range(doc_counter, doc_counter + batch_size))\n",
        "    for vec, page_num, doc in zip(batch_vecs, doc_idxs, batch.iterrows()):\n",
        "      vector_index_chunk.add_item(page_num, vec)\n",
        "      docs[page_num] = doc[1][\"_id\"]\n",
        "    doc_counter += batch_size\n",
        "    \n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 37281/37281 [00:16<00:00, 2230.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oeBqoE8e-scg"
      },
      "source": [
        "## Building an index of semantic vectors\n",
        "\n",
        "We use the [Annoy](https://github.com/spotify/annoy) library---to efficiently look up results from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoB9I9Pe4IT",
        "colab_type": "code",
        "outputId": "71746632-7e7a-48d5-ae06-8b4b8289a269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vector_index_chunk.build(10) # 10 trees\n",
        "vector_index_chunk.save('nyc_docs_annoy_small.bin') # you could save this and skip the step above, if you'd like"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRVX_CFopDyy",
        "colab_type": "text"
      },
      "source": [
        "What's indexed in Annoy is a meaningless set of 512 numbers for each sentence. Computers can sort of understand this, but humans can't. So we load up into memory the list of all the sentences, so we can print those as the result.\n",
        "\n",
        "This demo uses a fairly small (5mb) set of documents. If you were using this in \"real life\" you'd probably want to use a database to hold onto these -- they'd be too big to hold in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxzNAzI6mwtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_texts = pd.read_json('nyc_docs-sentences15.json', lines=True);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kg9cw0S2_ntQ"
      },
      "source": [
        "## Verify that the semantic-similarity search engine works\n",
        "\n",
        "Let's search for some stuff!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dxu66S8wJIG9"
      },
      "source": [
        "*   Try a few different sample sentences\n",
        "*   Try changing the number of returned results (they are returned in order of similarity)\n",
        "\n",
        "Once you've tried it out a bit, click the menu button to the left, and click Form -> Show Code to see what this is doing under the hood.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "_EFSd65B_mq8",
        "outputId": "47b19fe6-8465-4e52-ddb1-85f621988052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "sample_query = 'the subway is very crowded now'  #@param [\"the subway is very crowded now\", \"Some neighborhoods don't have access to healthy fruits and vegetables.\", \"homelessness is up\"] {allow-input: true}\n",
        "num_results = 10  #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
        "\n",
        "search_results = vector_index_chunk.get_nns_by_vector(query_embedding, n=num_results)\n",
        "\n",
        "print('sentences similar to: \"{}\"\\n'.format(sample_query))\n",
        "# search_results\n",
        "\n",
        "for idx, result_idx in enumerate(search_results):\n",
        "  page_num = docs[result_idx]\n",
        "  text = doc_texts[(doc_texts[\"_id\"] == page_num)][\"text\"].iloc[0]\n",
        "  print(f\"{page_num}: {text}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences similar to: \"the subway is very crowded now\"\n",
            "\n",
            "p2713: JeremyŠcan you clarify facts on capacity increase?\n",
            "p2713: JeremyŠcan you clarify facts on capacity increase?\n",
            "p1382: The MTA Capital program is important.\n",
            "p1393: MTA Performance Dashboard The bottom line: Subway riders shoulder a far larger burden than suburban riders, and one of the largest burdens of any system in the nation.\n",
            "p1402: State operating assistance.\n",
            "p1731: Manhattan, the ferry will save you 10 minutes each way relative to the subway!\n",
            "p2687: Terminal. The upgrades are part of a new development\n",
            "p2714: That starts with two new subway entrances.\n",
            "p2675: From:Grace, Melissa To:Norvell, Wiley ; Jeremy Soffin ; Schwartz, Regina ; DeLoach, Michael Subject:RE: can you send me whatever you have on 1 Vanderbilt groundbreaking?\n",
            "p1665: Later, the Mayor will create and deliver food packages with Joel Berg of NYC Coalition Against Hunger.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6YvctORJWgS",
        "colab_type": "text"
      },
      "source": [
        "## Wait, how did that work?\n",
        "\n",
        "### Nearest neighbors -- it's what it sounds like.\n",
        "\n",
        "When is a sentence \"similar\" to another?\n",
        "\n",
        "Remember those 512-dimensional vectors? We're treating two sentences as similar if their vectors are close together. Our search results are \"nearest neighbors,\" which is what it sounds like.\n",
        "\n",
        "Imagine the vectors were just three dimensions and we had four sentences, encoded as:\n",
        "\n",
        "1. [1, 2, 1]\n",
        "2. [100, 600, -12]\n",
        "3. [5, 7, 3]\n",
        "4. [-50, 1, -5798]\n",
        "\n",
        "Which sentence is probably the most similar to sentence #1?\n",
        "\n",
        "\"Annoy\" is a library that makes this easier to calculate quickly for hundreds of thousands of sentences. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "311mS63qpp84"
      },
      "source": [
        "**Copyright 2019 The TensorFlow Hub Authors and Quartz.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "JMyTNwSJGGWg",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors and Quartz All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}