{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search videos using fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We want to find moments when a bicycle appears in a video.\n",
    "\n",
    "### Transfer learning with a pre-trained model\n",
    "\n",
    "This notebook assumes you have gone through the steps of the pervious notebook in this folder, `dd-sorting-images-with-fastai`.\n",
    "\n",
    "## The Plan\n",
    "\n",
    "The goal is to see if we can spot something specific in a video. This would be useful especially if you had hours of video you didn't have time to watch.\n",
    "\n",
    "In our case, we'll try to spot bicycles in this video:\n",
    "\n",
    "Our plan is:\n",
    "\n",
    "- Download a computer-vision model pre-trained on 14 million images\n",
    "- Further train that model on images containing \"yes bikes\" and \"no bikes\"\n",
    "- Chop up our video into frames\n",
    "- Use the model to detect bikes in frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This notebook is based on the early lessons in [Practical Deep Learning for Coders](https://course.fast.ai/), taught online by Jeremy Howard. I **highly** recommend this free online course. \n",
    "\n",
    "-- John Keefe, [Quartz](https://qz.com), October 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those using Google Colaboratory ...\n",
    "\n",
    "Be aware that Google Colab instances are ephemeral -- they vanish *Poof* when you close them, or after a period of sitting idle (currently 90 minutes), or if you use one for more than 12 hours.\n",
    "\n",
    "If you're using Google Colaboratory, be sure to set your runtime to \"GPU\" which speeds up your notebook for machine learning:\n",
    "\n",
    "![change runtime](https://qz-aistudio-public.s3.amazonaws.com/workshops/notebook_images/change_runtime_2.jpg)\n",
    "![pick gpu](https://qz-aistudio-public.s3.amazonaws.com/workshops/notebook_images/pick_gpu_2.jpg)\n",
    "\n",
    "Then run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ALL GOOGLE COLAB USERS RUN THIS CELL\n",
    "\n",
    "## This runs a script that installs fast.ai\n",
    "!curl -s https://course.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those _not_ using Google Colaboratory ...\n",
    "\n",
    "This section is just for people who decide to use one of the notebooks on a system other than Google Colaboartory. \n",
    "\n",
    "Those people should run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NON-COLABORATORY USERS SHOULD RUN THIS CELL\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "!sudo apt --yes install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everybody do this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone needs to run the next cell, which initializes the Python libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## AND *EVERYBODY* SHOULD RUN THIS CELL\n",
    "import glob\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "from IPython.display import Image as Show\n",
    "import fastai\n",
    "print(f'fastai: {fastai.__version__}')\n",
    "print(f'cuda: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the video and the \"yes bikes\" and \"no bikes\" images, which are in the folders `bike` and `nobike` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget -N https://qz-aistudio-public.s3.amazonaws.com/workshops/bikes_data.zip --quiet\n",
    "!unzip -q bikes_data.zip\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a subdirectory called `bikes_data` which contains two folders (`bike` and `nobike`) and the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ls bikes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ls bikes_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls bikes_data/images/bike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a looks at one ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Show(filename='bikes_data/images/bike/IMG_1494.JPG', width=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebooks aren't great at playing videos, so I posted `bikes_data/intersection_movie.mov` on [Vimeo](https://vimeo.com/354069170)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load our image data in a format that's ready for the training code. We do that with fast.ai's data block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = Path('./bikes_data/images') ## The path for our data\n",
    "\n",
    "my_transforms = get_transforms() # get_transforms() is the default transform set\n",
    "\n",
    "data = (ImageList.from_folder(data_path) #Where to find the data? -> in path and its subfolders\n",
    "        .split_by_rand_pct() #How to split in train/valid? -> do it *randomly* (Not by folder)\n",
    "        .label_from_folder() #How to label? -> depending on the folder of the filenames\n",
    "        .transform(my_transforms, size=(224,224)) #Data transforms applied, size of images shrink to 224\n",
    "        .databunch(bs=48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's be sure to check our classes\n",
    "print(data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Once again, we'll be raining a computer-vision model by starting with a pre-trained model called ResNet30. With fast.ai we can infuse this model with our images (and their labels). This takes advantage of all of resnet34's \"knowledge\" of image-detection and tacks on our particular problem. This technique is called \"transfer learning.\"\n",
    "\n",
    "First we load our `data` and `model.resnet34` together into a training model known as a \"learner.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, models.resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train for 6 epochs (6 cycles through all our data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are we doing?\n",
    "\n",
    "So far, we have a pretty good error rate. It's actually possible to do even better, but we'll stick with this for now.\n",
    "\n",
    "We can take a look to see where the model was most confused, and whether what the model predicted was reasonable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "losses,idxs = interp.top_losses()\n",
    "len(data.valid_ds)==len(losses)==len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, heatmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the situations in which it was most confused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(4,4), dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save what we have in case we mess it up later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Show(filename='bikes_data/never_seen_image.jpg', width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = open_image('bikes_data/never_seen_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_class, pred_idx, outputs = learn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search our video\n",
    "\n",
    "Now we'll apply our model to our video! First we need to turn the video into a bunch of images using `ffmpeg`, which we loaded at the beginning of this notebook.\n",
    "\n",
    "We'll be using a very cool, all-purpose video tool called \"[ffmpeg](https://www.ffmpeg.org/).\" Check out the [ffmpeg documentation]() for more about all the things it can do, and the commands you can use. I also googled around to figure out the best way to use ffmpeg to split videos into images.\n",
    "\n",
    "The next, admittedly cryptic command, pulls out a `.jpg` image from the video every second (the `fps` or frames per second is set to `1`). It also names the images using the format \"myframe0000.jpg\" ... with the four zeros increasing with every frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ffmpeg -i bikes_data/intersection_movie.mov -vf fps=1 -hide_banner -loglevel panic -vsync 0 myframe%04d.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see what we have\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a list of all the \"myframe\" file names\n",
    "file_list = sorted(glob.glob('myframe*.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    image = open_image(file)\n",
    "    pred_class,pred_idx,outputs = learn.predict(image)\n",
    "        \n",
    "    if str(pred_class) == \"bike\" and outputs[0] > 0.85:\n",
    "        print(f'Bike detected in {file} with confidence {outputs[0]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Show(filename='myframe0025.jpg', width=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Doing even better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations apply visual fun to the training images to mix things up a bit and help make the model more _generalizable_ -- that is, for example, seeing more kinds of images of bikes than the images we have by changing the lighting, the cropping, the rotation, and the flip of the images. \n",
    "\n",
    "There is a really good description of transformations [in this blog post](https://medium.com/@pierre_guillou/data-augmentation-by-fastai-v1-84ca04bea302). The fast.ai transformation documentation is a little harder to follow if you're not a coder, [but it's here](https://docs.fast.ai/vision.transform.html).\n",
    "\n",
    "### The *get_transforms()* set\n",
    "\n",
    "We've been using `get_transforms()` to adjust our images so far. That is actually a _set_ of transformations, [detailed here](https://docs.fast.ai/vision.transform.html#get_transforms), which do the following to the training images:\n",
    "\n",
    "- flip the images horizontally with a 50% chance\n",
    "- don't flip the images vertically\n",
    "- rotate the image between -10 and +10 degrees with a 75% chance\n",
    "- zoom into the image up to 110% with a 75% chance\n",
    "- change the brightness and contrast by 20% with a 75% chance\n",
    "- warp the image between -20% and +20% with a chance 75% chance\n",
    "\n",
    "Those settings have worked well for a wide range of tasks.\n",
    "\n",
    "These `get_transforms()` defaults are:\n",
    "\n",
    "```\n",
    "my_transforms = (get_transforms(\n",
    "                   do_flip=True, \n",
    "                   flip_vert=False,\n",
    "                   max_rotate=10.0, \n",
    "                   max_zoom=1.1, \n",
    "                   max_lighting=0.2, \n",
    "                   max_warp:float=0.2, \n",
    "                   p_affine=0.75, \n",
    "                   p_lighting=0.75) )\n",
    "```\n",
    "\n",
    "Note that `p_affine` is the probability a zoom or warp will happen, and `p_lighting` is the probability a lighting change will happen.\n",
    "\n",
    "If you want to **change** these values, say to allow vertical flipping and increase the rotation, you can just declare changes in those parameters:\n",
    "\n",
    "```\n",
    "my_transforms = (get_transforms(\n",
    "                   flip_vert=True,\n",
    "                   max_rotate=25.0) )\n",
    "```\n",
    "\n",
    "You can also **add** transformations to this set:\n",
    "\n",
    "```\n",
    "extra_transformations = [\n",
    "                [zoom_crop(scale=(1.0,1.50), do_rand=False)],  \n",
    "                [contrast(scale=(0.5,2.0), p=0.75 ] ]\n",
    "\n",
    "my_transforms = (get_transforms(\n",
    "                   flip_vert=True,\n",
    "                   max_rotate=25.0\n",
    "                   xtra_tfms=extra_tranformations) )\n",
    "```\n",
    "\n",
    "Let's modify ours to minimize rotation, since we don't get many images of bikes going uphill dramatically, and add some zoom, since most of our examples are in the middle of the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using all the get_transforms defaults except a couple which we're changing\n",
    "my_transforms = (get_transforms(\n",
    "                   max_rotate=5.0, \n",
    "                   max_zoom=2.0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading same images as \"more\" images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"learn\" variable still has all of our trained information – including the structure of the model and the model math, known as \"weights.\" We can add more data to it.\n",
    "\n",
    "We'll add our new transforms, _and_ we'll change the size of the incoming images. To the model, these will look like _brand new images it has never seen before_. \n",
    "\n",
    "That's because in the training process, our convolutional neural network looks at small groups of pixels at a time. If we increase the size of the image, it's looking at _brand new clusters of pixels_.\n",
    "\n",
    "Originally we had 244x244 images, each of which which have 59,536 pixels. If we boost that to 512x512, that's 262,144 pixels per image ... and many more small groups to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a new data bunch, but size the images to 512 pixes square\n",
    "data2 = (ImageList.from_folder(data_path) #Where to find the data? -> in path and its subfolders\n",
    "        .split_by_rand_pct() #How to split in train/valid? -> do it *randomly* (Not by folder)\n",
    "        .label_from_folder() #How to label? -> depending on the folder of the filenames\n",
    "        .transform(my_tranforms, size=(512,512)) #Data transforms applied, size of images shrink to 224\n",
    "        .databunch(bs=12)) # We lower the batch size so we don't overwhelm the computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put this new data into the learner ...\n",
    "learn.data = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freeze what we've trained before\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train some more\n",
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSOgwF1ybVyh"
   },
   "source": [
    "## Saving Models to Google Drive\n",
    "\n",
    "At present, your Google Colaboratory Notebook disappears when you close it — along with all of your data and the values of the variables we set. That includes the model, which is called `learn`. \n",
    "\n",
    "If you'd like to save your model to your Google Drive in a folder called \"ai-workshop,\" run the following cell and grant the permissions it requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_dir = \"/content/gdrive/My Drive/\"\n",
    "base_dir = root_dir + 'ai-workshop/bike_video_models/'\n",
    "save_path = Path(base_dir)\n",
    "save_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line will save everything we need for predictions to a file to your Google Drive in the `ai-workshops` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.export(save_path/\"export.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, to load the model into your code, connect to your Google drive using the same block above that starts `from google.colab import drive ...` and then run this:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from the 'export.pkl' file on your Google Drive\n",
    "learn = load_learner(save_path)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
